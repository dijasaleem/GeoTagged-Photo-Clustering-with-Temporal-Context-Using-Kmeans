{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "7iPTlzmji-8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrHe43tT9s8w"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics.pairwise import haversine_distances\n",
        "from math import radians\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF_Ez4XDAonF",
        "outputId": "472e5ebd-7ac4-43e8-f878-052b1570b4d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=4fe359a2f4e57dd7c26b01617906b23d86c657c83a6eb8e2633aba307e13c862\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 121920 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u402-ga-2ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u402-ga-2ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u402-ga-2ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "metadata": {
        "id": "2PFmCVAzAsbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf().set(\"spark.ui.port\",\"4050\")\n",
        "sc = pyspark.SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "5SeOY2cjAyZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Geotagged Photo Analysis\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "5fGEnLMeAVXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**\n",
        "1. filter on the basis of london bounding box\n",
        "2. convert longtiude latitudes to radians\n",
        "3. drop where location is null\n",
        "4. extract season and part of day using \"taken\" attribute of photos"
      ],
      "metadata": {
        "id": "GDHLxL97EFBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/content/london_20k.csv\")\n",
        "bbox = (-0.5282454933, 51.2798389504, 0.2921446401, 51.6900253946)\n",
        "filtered_df = df.filter(\n",
        "    (df['lat'] >= bbox[1]) & (df['lon'] >= bbox[0]) &\n",
        "    (df['lat'] <= bbox[3]) & (df['lon'] <= bbox[2])\n",
        ")\n",
        "\n",
        "# Remove rows where latitude or longitude is missing\n",
        "filtered_df = filtered_df.dropna(subset=['lat', 'lon'])\n"
      ],
      "metadata": {
        "id": "RYVyz34696tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"lat_rad\", radians(\"lat\"))\n",
        "df = df.withColumn(\"lon_rad\", radians(\"lon\"))"
      ],
      "metadata": {
        "id": "KZU2bpV-IXAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_timestamp\n",
        "\n",
        "# Converting 'taken' to datetime format\n",
        "filtered_df = filtered_df.withColumn(\"taken\", to_timestamp(col(\"taken\")))\n"
      ],
      "metadata": {
        "id": "CHFc09H0-l4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import month, when, hour\n",
        "\n",
        "# Helper function to determine the season\n",
        "def get_season(month):\n",
        "    return when((month >= 3) & (month <= 5), \"Spring\") \\\n",
        "           .when((month >= 6) & (month <= 8), \"Summer\") \\\n",
        "           .when((month >= 9) & (month <= 11), \"Fall\") \\\n",
        "           .otherwise(\"Winter\")\n",
        "\n",
        "# Adding \"season\" based on month of 'taken'\n",
        "df = df.withColumn(\"month\", month(\"taken\"))\n",
        "df = df.withColumn(\"season\", get_season(col(\"month\")))\n",
        "\n",
        "# Helper function to determine the time of the day\n",
        "def get_part_of_day(hour):\n",
        "    return when((hour >= 6) & (hour < 12), \"Morning\") \\\n",
        "           .when((hour >= 12) & (hour < 18), \"Afternoon\") \\\n",
        "           .otherwise(\"Night\")  # Accounts for hours between 18:00 and 6:00\n",
        "\n",
        "# Adding \"time of the day\" based on hour of 'taken'\n",
        "df = df.withColumn(\"hour\", hour(\"taken\"))\n",
        "df = df.withColumn(\"part_of_day\", get_part_of_day(col(\"hour\")))\n"
      ],
      "metadata": {
        "id": "Fk6BYFexEtXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsohmstgE63b",
        "outputId": "62a305a6-9d58-4feb-f9fe-b1318f9b1cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+------+-----------+--------------------+--------------------+--------------------+-----+-----------+-----------+------+---------+-------------------+------------------+--------------------+-----+------+----+-----------+\n",
            "|   photo_id|       owner|gender| occupation|               title|         description|                tags|faves|        lat|        lon|u_city|u_country|              taken|           lat_rad|             lon_rad|month|season|hour|part_of_day|\n",
            "+-----------+------------+------+-----------+--------------------+--------------------+--------------------+-----+-----------+-----------+------+---------+-------------------+------------------+--------------------+-----+------+----+-----------+\n",
            "|12056545693|78191777@N00|     1|Producer/DJ|                près|\"<i>Let us draw n...|clapham, london, ...|    0|51.46516300|-0.12908500| 30307|      USA|2014-01-20 15:19:44|0.8982365444255623|-0.00225295826493...|    1|Winter|  15|  Afternoon|\n",
            "|12453639663|41087279@N00|     1| Accountant|DSC_4241 Chyna Wh...|Chyna Whyne from ...|chyna, whyne, fro...|    1|51.52767200|-0.08364800|London|  England|2014-02-09 23:05:35|0.8993275322876916|-0.00145993301270...|    2|Winter|  23|      Night|\n",
            "|13185773995|41087279@N00|     1| Accountant|DSC_6178 Flirt 69...|Flirt 69 Birthday...|flirt, 69, birthd...|    0|51.52767200|-0.08364800|London|  England|2014-03-14 22:24:02|0.8993275322876916|-0.00145993301270...|    3|Spring|  22|      Night|\n",
            "|13295046445|30625665@N00|     1|         NA|        Bank Limited|BREAKING NEWS My ...|bank, ex, cash, p...|    5|51.51304200|-0.08922100|    NA|       NA|2014-03-20 09:13:03|0.8990721906181248|-0.00155720021192...|    3|Spring|   9|    Morning|\n",
            "|13357656115|41087279@N00|     1| Accountant|DSC_6743 Ray Esta...|Ray Estaire Live ...|ray, estaire, jaz...|    1|51.52767200|-0.08364800|London|  England|2014-03-21 23:18:39|0.8993275322876916|-0.00145993301270...|    3|Spring|  23|      Night|\n",
            "+-----------+------------+------+-----------+--------------------+--------------------+--------------------+-----+-----------+-----------+------+---------+-------------------+------------------+--------------------+-----+------+----+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clustering**\n",
        "1. step of preprocessing\n",
        "2. clustering on the basis of location and temporal context\n",
        "3. KMeans"
      ],
      "metadata": {
        "id": "eAvsZci7FLmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure there are no nulls in the columns used by VectorAssembler\n",
        "df = df.na.drop(subset=[\"lat_rad\", \"lon_rad\", \"season\", \"part_of_day\"])\n",
        "\n",
        "# Indexing categorical columns to convert to numeric form\n",
        "indexer_season = StringIndexer(inputCol=\"season\", outputCol=\"season_idx\")\n",
        "indexer_part_of_day = StringIndexer(inputCol=\"part_of_day\", outputCol=\"part_of_day_idx\")\n",
        "\n",
        "# Assemble all features into one vector column\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"lat_rad\", \"lon_rad\", \"season_idx\", \"part_of_day_idx\"],\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"skip\"\n",
        ")\n",
        "\n",
        "# Define the KMeans model\n",
        "kmeans = KMeans(k=5, seed=1)\n",
        "\n",
        "# Build the pipeline\n",
        "pipeline = Pipeline(stages=[indexer_season, indexer_part_of_day, assembler, kmeans])\n",
        "\n",
        "# Fit the pipeline on the data\n",
        "model = pipeline.fit(df)\n",
        "\n",
        "# Transform the data\n",
        "df_clustered = model.transform(df)\n",
        "\n",
        "# Show cluster centers and predicted clusters\n",
        "centers = model.stages[-1].clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)\n",
        "\n",
        "df_clustered.select(\"photo_id\", \"lat\", \"lon\", \"season\", \"part_of_day\", \"prediction\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go0p898jK1zx",
        "outputId": "cdcf5b0f-41c7-4786-c473-48f3a226b4b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers: \n",
            "[ 8.98876359e-01 -2.62555558e-03  3.00000000e+00  0.00000000e+00]\n",
            "[ 0.89894011 -0.00235188  1.38314945  0.        ]\n",
            "[ 0.89896228 -0.00300653  0.          0.20269216]\n",
            "[0.89274044 0.00719341 2.33877454 1.3087296 ]\n",
            "[ 0.89898836 -0.00216751  0.66467554  1.56910128]\n",
            "+-----------+-----------+-----------+------+-----------+----------+\n",
            "|   photo_id|        lat|        lon|season|part_of_day|prediction|\n",
            "+-----------+-----------+-----------+------+-----------+----------+\n",
            "|12056545693|51.46516300|-0.12908500|Winter|  Afternoon|         0|\n",
            "|12453639663|51.52767200|-0.08364800|Winter|      Night|         3|\n",
            "|13185773995|51.52767200|-0.08364800|Spring|      Night|         4|\n",
            "|13295046445|51.51304200|-0.08922100|Spring|    Morning|         4|\n",
            "|13357656115|51.52767200|-0.08364800|Spring|      Night|         4|\n",
            "+-----------+-----------+-----------+------+-----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clustered.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0k7w-1oLLoL",
        "outputId": "ffb0895c-0522-4ff4-8bbb-8f7164c30a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+------+-----------+--------------------+--------------------+--------------------+-----+-----------+-----------+------+---------+-------------------+------------------+--------------------+-----+------+----+-----------+----------+---------------+--------------------+----------+\n",
            "|   photo_id|       owner|gender| occupation|               title|         description|                tags|faves|        lat|        lon|u_city|u_country|              taken|           lat_rad|             lon_rad|month|season|hour|part_of_day|season_idx|part_of_day_idx|            features|prediction|\n",
            "+-----------+------------+------+-----------+--------------------+--------------------+--------------------+-----+-----------+-----------+------+---------+-------------------+------------------+--------------------+-----+------+----+-----------+----------+---------------+--------------------+----------+\n",
            "|12056545693|78191777@N00|     1|Producer/DJ|                près|\"<i>Let us draw n...|clapham, london, ...|    0|51.46516300|-0.12908500| 30307|      USA|2014-01-20 15:19:44|0.8982365444255623|-0.00225295826493...|    1|Winter|  15|  Afternoon|       3.0|            0.0|[0.89823654442556...|         0|\n",
            "|12453639663|41087279@N00|     1| Accountant|DSC_4241 Chyna Wh...|Chyna Whyne from ...|chyna, whyne, fro...|    1|51.52767200|-0.08364800|London|  England|2014-02-09 23:05:35|0.8993275322876916|-0.00145993301270...|    2|Winter|  23|      Night|       3.0|            2.0|[0.89932753228769...|         3|\n",
            "|13185773995|41087279@N00|     1| Accountant|DSC_6178 Flirt 69...|Flirt 69 Birthday...|flirt, 69, birthd...|    0|51.52767200|-0.08364800|London|  England|2014-03-14 22:24:02|0.8993275322876916|-0.00145993301270...|    3|Spring|  22|      Night|       1.0|            2.0|[0.89932753228769...|         4|\n",
            "|13295046445|30625665@N00|     1|         NA|        Bank Limited|BREAKING NEWS My ...|bank, ex, cash, p...|    5|51.51304200|-0.08922100|    NA|       NA|2014-03-20 09:13:03|0.8990721906181248|-0.00155720021192...|    3|Spring|   9|    Morning|       1.0|            1.0|[0.89907219061812...|         4|\n",
            "|13357656115|41087279@N00|     1| Accountant|DSC_6743 Ray Esta...|Ray Estaire Live ...|ray, estaire, jaz...|    1|51.52767200|-0.08364800|London|  England|2014-03-21 23:18:39|0.8993275322876916|-0.00145993301270...|    3|Spring|  23|      Night|       1.0|            2.0|[0.89932753228769...|         4|\n",
            "+-----------+------------+------+-----------+--------------------+--------------------+--------------------+-----+-----------+-----------+------+---------+-------------------+------------------+--------------------+-----+------+----+-----------+----------+---------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**kmeans by finding optimal values of k**"
      ],
      "metadata": {
        "id": "HR3fg41BKfIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml import Pipeline\n"
      ],
      "metadata": {
        "id": "TKNslBFtIv75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.na.drop(subset=[\"lat_rad\", \"lon_rad\", \"season\", \"part_of_day\"])  # Ensure no nulls\n",
        "\n",
        "# Indexing categorical columns\n",
        "indexer_season = StringIndexer(inputCol=\"season\", outputCol=\"season_idx\")\n",
        "indexer_part_of_day = StringIndexer(inputCol=\"part_of_day\", outputCol=\"part_of_day_idx\")\n",
        "\n",
        "# Assemble all features into one vector column\n",
        "assembler = VectorAssembler(inputCols=[\"lat_rad\", \"lon_rad\", \"season_idx\", \"part_of_day_idx\"], outputCol=\"features\")\n",
        "stages = [indexer_season, indexer_part_of_day, assembler]\n",
        "\n",
        "# Define a range of k to evaluate\n",
        "k_range = range(9, 20)\n",
        "silhouette_scores = []\n",
        "best_k = 2\n",
        "best_score = -1\n",
        "\n",
        "for k in k_range:\n",
        "\n",
        "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\").setPredictionCol(\"prediction\")\n",
        "    pipeline = Pipeline(stages=stages + [kmeans])\n",
        "    model = pipeline.fit(df)\n",
        "    predictions = model.transform(df)\n",
        "    evaluator = ClusteringEvaluator()\n",
        "    silhouette = evaluator.evaluate(predictions)\n",
        "    silhouette_scores.append(silhouette)\n",
        "\n",
        "    print(f\"Silhouette score for k={k}: {silhouette}\")\n",
        "\n",
        "    if silhouette > best_score:\n",
        "        best_k = k\n",
        "        best_score = silhouette\n",
        "\n",
        "\n",
        "print(f\"Best k by Silhouette score: {best_k}\")\n",
        "final_kmeans = KMeans().setK(best_k).setSeed(1).setFeaturesCol(\"features\").setPredictionCol(\"prediction\")\n",
        "final_pipeline = Pipeline(stages=stages + [final_kmeans])\n",
        "final_model = final_pipeline.fit(df)\n",
        "df_clustered = final_model.transform(df)\n",
        "\n",
        "# Show results\n",
        "df_clustered.select(\"photo_id\", \"lat\", \"lon\", \"season\", \"part_of_day\", \"prediction\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd2OEDvAIouV",
        "outputId": "ed37c71e-f3ff-4be9-9e14-e687cb111c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette score for k=9: 0.8509387733207695\n",
            "Silhouette score for k=10: 0.8678546835663999\n",
            "Silhouette score for k=11: 0.9632128108464526\n",
            "Silhouette score for k=12: 0.9958849285059251\n",
            "Silhouette score for k=13: 0.9868955892971455\n",
            "Silhouette score for k=14: 0.8925083212036053\n",
            "Silhouette score for k=15: 0.8792812477325019\n",
            "Silhouette score for k=16: 0.8531197800347217\n",
            "Silhouette score for k=17: 0.8718570378185633\n",
            "Silhouette score for k=18: 0.876198805874861\n",
            "Silhouette score for k=19: 0.8803832516186971\n",
            "Best k by Silhouette score: 12\n",
            "+-----------+-----------+-----------+------+-----------+----------+\n",
            "|   photo_id|        lat|        lon|season|part_of_day|prediction|\n",
            "+-----------+-----------+-----------+------+-----------+----------+\n",
            "|12056545693|51.46516300|-0.12908500|Winter|  Afternoon|         6|\n",
            "|12453639663|51.52767200|-0.08364800|Winter|      Night|        11|\n",
            "|13185773995|51.52767200|-0.08364800|Spring|      Night|         9|\n",
            "|13295046445|51.51304200|-0.08922100|Spring|    Morning|         7|\n",
            "|13357656115|51.52767200|-0.08364800|Spring|      Night|         9|\n",
            "+-----------+-----------+-----------+------+-----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Location time database**\n",
        "1. Filter out photos taken within 3 hours by the same user at the same location\n",
        "2. Filter out users with fewer than three distinct visits\n",
        "3. include most common season and part of day for visits"
      ],
      "metadata": {
        "id": "WZ0GSH1GPryE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate time difference and flag duplicates\n",
        "df_clustered = df_clustered.withColumn(\"timestamp_unix\", unix_timestamp(\"taken\"))\n",
        "window_spec = Window.partitionBy(\"owner\", \"prediction\").orderBy(\"timestamp_unix\")\n",
        "df_clustered = df_clustered.withColumn(\"time_diff\", abs(col(\"timestamp_unix\") - lag(\"timestamp_unix\", 1).over(window_spec)))\n",
        "df_clustered = df_clustered.filter((col(\"time_diff\") > 10800) | col(\"time_diff\").isNull())\n",
        "\n",
        "# Count distinct clusters per user\n",
        "user_visit_counts = df_clustered.groupBy(\"owner\").agg(countDistinct(\"prediction\").alias(\"distinct_clusters\"))\n",
        "eligible_users = user_visit_counts.filter(col(\"distinct_clusters\") >= 3)\n",
        "\n",
        "# Join eligible users back to the clustered data\n",
        "df_final = df_clustered.join(eligible_users, \"owner\", \"inner\")\n",
        "\n",
        "# Group by cluster to get centroids and count visits\n",
        "cluster_details = df_final.groupBy(\"prediction\").agg(\n",
        "    mean(col(\"lat\")).alias(\"cent_lat\"),\n",
        "    mean(col(\"lon\")).alias(\"cent_lon\"),\n",
        "    count(\"photo_id\").alias(\"visit_count\")\n",
        ")\n",
        "\n",
        "# Calculate the most common 'season' and 'part_of_day' for each cluster\n",
        "common_time_details = df_final.groupBy(\"prediction\", \"season\", \"part_of_day\").agg(\n",
        "    count(\"photo_id\").alias(\"count\")\n",
        ").withColumn(\"rank_season\", rank().over(Window.partitionBy(\"prediction\", \"season\").orderBy(col(\"count\").desc())))\n",
        "most_common_season = common_time_details.filter(col(\"rank_season\") == 1).drop(\"count\", \"rank_season\", \"part_of_day\")\n",
        "\n",
        "common_time_details = common_time_details.withColumn(\"rank_part_of_day\", rank().over(Window.partitionBy(\"prediction\", \"part_of_day\").orderBy(col(\"count\").desc())))\n",
        "most_common_partofday = common_time_details.filter(col(\"rank_part_of_day\") == 1).drop(\"count\", \"rank_part_of_day\")\n",
        "\n",
        "\n",
        "location_time_database = df_final.join(cluster_details, \"prediction\")\n",
        "location_time_database= location_time_database.join(most_common_season, [\"prediction\", \"season\"])\n",
        "most_common_partofday = most_common_partofday.drop(\"season\")\n",
        "location_time_database= location_time_database.join(most_common_partofday, [\"prediction\", \"part_of_day\"]).select(\n",
        "    col(\"prediction\").alias(\"location_id\"),\n",
        "    \"photo_id\",\n",
        "    \"owner\",\n",
        "    \"lat\",\n",
        "    \"lon\",\n",
        "    \"taken\",\n",
        "    \"cent_lat\",\n",
        "    \"cent_lon\",\n",
        "    \"season\",\n",
        "    \"part_of_day\"\n",
        ")\n",
        "location_time_database.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag40f-3WPwtB",
        "outputId": "5bac96ff-ac60-4252-f367-1f271fa47c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- location_id: integer (nullable = false)\n",
            " |-- photo_id: string (nullable = true)\n",
            " |-- owner: string (nullable = true)\n",
            " |-- lat: string (nullable = true)\n",
            " |-- lon: string (nullable = true)\n",
            " |-- taken: string (nullable = true)\n",
            " |-- cent_lat: double (nullable = true)\n",
            " |-- cent_lon: double (nullable = true)\n",
            " |-- season: string (nullable = false)\n",
            " |-- part_of_day: string (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformation**"
      ],
      "metadata": {
        "id": "D3HT21_njIC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, max, min, lit\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.recommendation import ALS\n",
        "\n",
        "\n",
        "# Step 1: Calculate visit frequencies\n",
        "visit_counts = location_time_database.groupBy(\"owner\", \"location_id\", \"season\", \"part_of_day\").count().withColumnRenamed(\"count\", \"frequency\")\n",
        "\n",
        "# Step 2: Normalize frequencies to a 1-5 scale\n",
        "# First, find global max and min frequencies to scale ratings between 1 and 5\n",
        "freq_stats = visit_counts.select(max(\"frequency\").alias(\"max_freq\"), min(\"frequency\").alias(\"min_freq\")).collect()\n",
        "max_freq = freq_stats[0][\"max_freq\"]\n",
        "min_freq = freq_stats[0][\"min_freq\"]\n",
        "\n",
        "# Apply Min-Max normalization\n",
        "visit_counts = visit_counts.withColumn(\"ratings\",\n",
        "                                       ((col(\"frequency\") - min_freq) / (max_freq - min_freq) * 4 + 1))\n",
        "\n",
        "#making owner attribute as type int\n",
        "user_indexer = StringIndexer(inputCol=\"owner\", outputCol=\"user_id_indexed\")\n",
        "indexer_model = user_indexer.fit(visit_counts)\n",
        "visit_counts_indexed = indexer_model.transform(visit_counts)\n",
        "als_input = visit_counts_indexed.select(\n",
        "    col(\"user_id_indexed\").alias(\"user_id\"),\n",
        "    col(\"location_id\"),\n",
        "    col(\"ratings\").cast(\"float\")\n",
        ")\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "(training_data, test_data) = als_input.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Now we can proceed to use the ALS model as in previous steps\n",
        "als = ALS(\n",
        "    userCol=\"user_id\",\n",
        "    itemCol=\"location_id\",\n",
        "    ratingCol=\"ratings\",\n",
        "    nonnegative=True,\n",
        "    implicitPrefs=False,\n",
        "    coldStartStrategy=\"drop\",\n",
        "    rank=10,\n",
        "    maxIter=10\n",
        ")\n",
        "\n",
        "# Fit the ALS model to the training data\n",
        "model = als.fit(training_data)\n"
      ],
      "metadata": {
        "id": "DcZ5xzvVlpOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**"
      ],
      "metadata": {
        "id": "YB30ukYa5M7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Root Mean Square Error**"
      ],
      "metadata": {
        "id": "mptB8Be-5Rvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "predictions = model.transform(test_data)\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"ratings\",\n",
        "                                predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root-mean-square error = \" + str(rmse))\n",
        "\n",
        "# Show example predictions\n",
        "predictions.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwBB0q2Tmyo0",
        "outputId": "9bda4615-b4b8-4bf7-b5a3-335151b174d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root-mean-square error = 0.4163377056277574\n",
            "+-------+-----------+-------+----------+\n",
            "|user_id|location_id|ratings|prediction|\n",
            "+-------+-----------+-------+----------+\n",
            "|    0.0|          2|    1.2| 2.6525476|\n",
            "|    0.0|          6|    2.0| 2.5043044|\n",
            "|    0.0|          8|    1.0| 2.2437298|\n",
            "|    1.0|          1|    2.8| 2.3076637|\n",
            "|    1.0|          7|    1.8| 1.9100902|\n",
            "+-------+-----------+-------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mean Absolute Error**"
      ],
      "metadata": {
        "id": "thFpZiOr5X0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mae_evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"ratings\", predictionCol=\"prediction\")\n",
        "mae = mae_evaluator.evaluate(predictions)\n",
        "print(\"Mean Absolute Error = \" + str(mae))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yqj8EYms5bby",
        "outputId": "38c819a6-22c4-4e06-f056-6ebf12adac28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error = 0.27722900972238035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approximate Mean Average Precision at n (MAP@n)**\n",
        "1. Generating top-n recommendations for each user.\n",
        "2. Determining if these recommendations are in the user's test set.\n",
        "3. Calculating precision at n for each user.\n",
        "4. Averaging these precisions."
      ],
      "metadata": {
        "id": "ynf072_s5nFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col, expr, explode, rank, desc\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Generate predictions for test data\n",
        "predictions = model.transform(test_data)\n",
        "predictions.show(5)\n",
        "\n",
        "# Evaluate the model by computing the RMSE on the test data\n",
        "rmse_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"ratings\", predictionCol=\"prediction\")\n",
        "rmse = rmse_evaluator.evaluate(predictions)\n",
        "print(\"Root-mean-square error = \" + str(rmse))\n",
        "\n",
        "# Evaluate the model by computing the MAE on the test data\n",
        "mae_evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"ratings\", predictionCol=\"prediction\")\n",
        "mae = mae_evaluator.evaluate(predictions)\n",
        "print(\"Mean Absolute Error = \" + str(mae))\n",
        "\n",
        "# Generate top 10 recommendations for all users\n",
        "user_recs = model.recommendForAllUsers(10)\n",
        "user_recs.show(5)\n",
        "\n",
        "# Explode the recommendations to flat format for easier processing\n",
        "recommendations = user_recs.select(\n",
        "    \"user_id\",\n",
        "    explode(\"recommendations\").alias(\"recommendation\")\n",
        ").select(\n",
        "    \"user_id\",\n",
        "    col(\"recommendation.location_id\").alias(\"location_id\"),\n",
        "    col(\"recommendation.rating\").alias(\"rating\")\n",
        ")\n",
        "\n",
        "# Join exploded recommendations with actual ratings for MAP calculation\n",
        "# Assume `predictions` has columns: user_id, location_id, prediction, and ratings\n",
        "true_positives = predictions.join(\n",
        "    recommendations,\n",
        "    [\"user_id\", \"location_id\"],\n",
        "    \"inner\"\n",
        ").select(\"user_id\", \"location_id\")\n",
        "\n",
        "# To calculate true positives within top k\n",
        "precision_at_k = {}\n",
        "for k in [5, 10, 15]:\n",
        "    top_k_predictions = predictions.withColumn(\"rank\", rank().over(Window.partitionBy(\"user_id\").orderBy(desc(\"prediction\"))))\n",
        "    filtered_predictions = top_k_predictions.filter(col(\"rank\") <= k)\n",
        "    tp_at_k = filtered_predictions.join(true_positives, [\"user_id\", \"location_id\"], \"inner\").groupBy(\"user_id\").agg(count(\"location_id\").alias(\"tp_at_k\"))\n",
        "    relevant = filtered_predictions.groupBy(\"user_id\").agg(count(\"location_id\").alias(\"total_relevant\"))\n",
        "    precision_k = tp_at_k.join(relevant, \"user_id\").selectExpr(\"user_id\", \"tp_at_k / total_relevant as precision_at_k\")\n",
        "    average_precision = precision_k.select(mean(\"precision_at_k\")).collect()[0][0]\n",
        "    precision_at_k[k] = average_precision\n",
        "\n",
        "# Print MAP@n results\n",
        "for k, precision in precision_at_k.items():\n",
        "    print(f\"Mean Average Precision at {k}: {precision}\")\n"
      ],
      "metadata": {
        "id": "TsNdC0IF8rCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e716e3-4e82-401b-d0bd-d986055fd6b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+-------+----------+\n",
            "|user_id|location_id|ratings|prediction|\n",
            "+-------+-----------+-------+----------+\n",
            "|    0.0|          2|    1.2| 2.6525476|\n",
            "|    0.0|          6|    2.0| 2.5043044|\n",
            "|    0.0|          8|    1.0| 2.2437298|\n",
            "|    1.0|          1|    2.8| 2.3076637|\n",
            "|    1.0|          7|    1.8| 1.9100902|\n",
            "+-------+-----------+-------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Root-mean-square error = 0.4163377056277574\n",
            "Mean Absolute Error = 0.27722900972238035\n",
            "+-------+--------------------+\n",
            "|user_id|     recommendations|\n",
            "+-------+--------------------+\n",
            "|      0|[{0, 3.8025756}, ...|\n",
            "|      1|[{6, 2.4007404}, ...|\n",
            "|      2|[{1, 2.4980018}, ...|\n",
            "|      3|[{1, 1.4435956}, ...|\n",
            "|      4|[{5, 1.2446896}, ...|\n",
            "+-------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Mean Average Precision at 5: 0.981505376344086\n",
            "Mean Average Precision at 10: 0.981505376344086\n",
            "Mean Average Precision at 15: 0.981505376344086\n"
          ]
        }
      ]
    }
  ]
}